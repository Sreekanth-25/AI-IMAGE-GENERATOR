{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PtYYsJsd_-am",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15ff4eeb-e598-4bce-df06-c68917bbbc61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m54.3/54.3 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m323.9/323.9 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install diffusers transformers accelerate torch torchvision gradio pillow xformers -q\n",
        "!pip install --upgrade gradio -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Advanced Stable Diffusion Image Generator - Fixed Version\n",
        "# Compatible with all Gradio versions + Fast Generation + Activity History\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import gradio as gr\n",
        "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "from datetime import datetime\n",
        "import json\n",
        "import threading\n",
        "import gc\n",
        "\n",
        "# ==================== STEP 1: INSTALL REQUIRED PACKAGES ====================\n",
        "# Run this cell first in Colab:\n",
        "\"\"\"\n",
        "!pip install --upgrade pip\n",
        "!pip install gradio==3.50.2\n",
        "!pip install diffusers transformers accelerate torch torchvision pillow xformers -q\n",
        "!pip install --upgrade huggingface_hub\n",
        "\"\"\"\n",
        "\n",
        "# Check Gradio version and use compatible syntax\n",
        "print(f\"Gradio version: {gr.__version__}\")\n",
        "\n",
        "class FastImageGenerator:\n",
        "    def __init__(self):\n",
        "        self.pipe = None\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "        self.model_loaded = False\n",
        "        self.generation_history = []\n",
        "        self.activity_log = []\n",
        "\n",
        "        print(f\"üöÄ Device: {self.device}\")\n",
        "        if self.device == \"cuda\":\n",
        "            print(f\"üìä GPU: {torch.cuda.get_device_name(0)}\")\n",
        "            print(f\"üíæ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "\n",
        "    def log_activity(self, action, details=\"\"):\n",
        "        \"\"\"Log user activity\"\"\"\n",
        "        activity = {\n",
        "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"action\": action,\n",
        "            \"details\": details\n",
        "        }\n",
        "        self.activity_log.append(activity)\n",
        "        if len(self.activity_log) > 50:  # Keep last 50 activities\n",
        "            self.activity_log.pop(0)\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load model with optimizations for speed\"\"\"\n",
        "        try:\n",
        "            self.log_activity(\"Model Loading\", \"Starting to load Stable Diffusion model\")\n",
        "\n",
        "            # Clear memory\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "            print(\"üì• Loading Stable Diffusion model...\")\n",
        "\n",
        "            # Load with optimizations\n",
        "            self.pipe = StableDiffusionPipeline.from_pretrained(\n",
        "                self.model_id,\n",
        "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
        "                safety_checker=None,\n",
        "                requires_safety_checker=False,\n",
        "                use_auth_token=False,\n",
        "                low_cpu_mem_usage=True\n",
        "            )\n",
        "\n",
        "            # Fast scheduler for quicker generation\n",
        "            self.pipe.scheduler = DPMSolverMultistepScheduler.from_config(\n",
        "                self.pipe.scheduler.config,\n",
        "                use_karras_sigmas=True\n",
        "            )\n",
        "\n",
        "            # Move to device\n",
        "            self.pipe = self.pipe.to(self.device)\n",
        "\n",
        "            # Enable all optimizations\n",
        "            if self.device == \"cuda\":\n",
        "                #self.pipe.enable_memory_efficient_attention()\n",
        "                self.pipe.enable_attention_slicing()\n",
        "                try:\n",
        "                    self.pipe.enable_xformers_memory_efficient_attention()\n",
        "                    print(\"‚úÖ XFormers optimization enabled\")\n",
        "                except:\n",
        "                    print(\"‚ö†Ô∏è XFormers not available, using standard attention\")\n",
        "\n",
        "            # Enable CPU offloading for large models\n",
        "            self.pipe.enable_model_cpu_offload()\n",
        "\n",
        "            self.model_loaded = True\n",
        "            self.log_activity(\"Model Loaded\", \"Successfully loaded and optimized model\")\n",
        "\n",
        "            return \"‚úÖ Model loaded successfully! Ready for fast generation üöÄ\"\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"‚ùå Error loading model: {str(e)}\"\n",
        "            self.log_activity(\"Model Load Error\", str(e))\n",
        "            print(f\"Model loading error: {e}\")\n",
        "            return error_msg\n",
        "\n",
        "    def generate_image(self, prompt, negative_prompt, width, height, steps, guidance_scale, seed, batch_size):\n",
        "        \"\"\"Generate images with optimized settings\"\"\"\n",
        "        if not self.model_loaded:\n",
        "            return None, \"‚ùå Please load the model first!\"\n",
        "\n",
        "        try:\n",
        "            self.log_activity(\"Image Generation\", f\"Prompt: {prompt[:50]}...\")\n",
        "\n",
        "            # Optimize parameters for speed\n",
        "            if steps > 25:\n",
        "                steps = 25  # Cap steps for faster generation\n",
        "\n",
        "            # Set seed\n",
        "            if seed == -1:\n",
        "                seed = random.randint(0, 2**32 - 1)\n",
        "\n",
        "            generator = torch.Generator(device=self.device).manual_seed(seed)\n",
        "\n",
        "            print(f\"üé® Generating with optimized settings...\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Generate with optimizations\n",
        "            with torch.autocast(self.device), torch.no_grad():\n",
        "                result = self.pipe(\n",
        "                    prompt=prompt,\n",
        "                    negative_prompt=negative_prompt,\n",
        "                    width=width,\n",
        "                    height=height,\n",
        "                    num_inference_steps=steps,\n",
        "                    guidance_scale=guidance_scale,\n",
        "                    generator=generator,\n",
        "                    num_images_per_prompt=batch_size,\n",
        "                    eta=0.0,  # Faster generation\n",
        "                    output_type=\"pil\"\n",
        "                )\n",
        "\n",
        "            end_time = time.time()\n",
        "            generation_time = end_time - start_time\n",
        "\n",
        "            # Store in history\n",
        "            generation_record = {\n",
        "                \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                \"prompt\": prompt,\n",
        "                \"negative_prompt\": negative_prompt,\n",
        "                \"seed\": seed,\n",
        "                \"steps\": steps,\n",
        "                \"guidance_scale\": guidance_scale,\n",
        "                \"size\": f\"{width}x{height}\",\n",
        "                \"time\": f\"{generation_time:.2f}s\",\n",
        "                \"batch_size\": batch_size\n",
        "            }\n",
        "            self.generation_history.append(generation_record)\n",
        "\n",
        "            # Keep only last 20 generations\n",
        "            if len(self.generation_history) > 20:\n",
        "                self.generation_history.pop(0)\n",
        "\n",
        "            self.log_activity(\"Image Generated\", f\"Generated {batch_size} images in {generation_time:.2f}s\")\n",
        "\n",
        "            images = result.images\n",
        "\n",
        "            # Format info\n",
        "            info_text = f\"‚úÖ Generated {len(images)} image(s) in {generation_time:.2f}s\\n\"\n",
        "            info_text += f\"üé≤ Seed: {seed}\\n\"\n",
        "            info_text += f\"‚öôÔ∏è Steps: {steps} | Guidance: {guidance_scale}\\n\"\n",
        "            info_text += f\"üìê Size: {width}x{height}\\n\"\n",
        "            info_text += f\"‚ö° Speed optimizations: ON\\n\"\n",
        "            info_text += f\"üí° Tip: Lower steps = faster generation!\"\n",
        "\n",
        "            return images, info_text\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"‚ùå Generation error: {str(e)}\"\n",
        "            self.log_activity(\"Generation Error\", str(e))\n",
        "            print(f\"Generation error: {e}\")\n",
        "            return None, error_msg\n",
        "\n",
        "    def get_random_prompt(self):\n",
        "        \"\"\"Generate random creative prompts\"\"\"\n",
        "        styles = [\"photorealistic\", \"digital art\", \"oil painting\", \"watercolor\", \"anime style\", \"cyberpunk\", \"fantasy art\"]\n",
        "        subjects = [\"mountain landscape\", \"futuristic city\", \"magical forest\", \"space station\", \"ancient temple\", \"underwater scene\"]\n",
        "        moods = [\"dramatic lighting\", \"soft glow\", \"vibrant colors\", \"golden hour\", \"neon lights\", \"mystical atmosphere\"]\n",
        "        quality = [\"highly detailed\", \"8k\", \"masterpiece\", \"stunning\", \"cinematic\"]\n",
        "\n",
        "        return f\"{random.choice(styles)} of {random.choice(subjects)} with {random.choice(moods)}, {random.choice(quality)}\"\n",
        "\n",
        "    def get_activity_history(self):\n",
        "        \"\"\"Get formatted activity history\"\"\"\n",
        "        if not self.activity_log:\n",
        "            return \"No activities yet\"\n",
        "\n",
        "        history_text = \"üìã Recent Activities:\\n\" + \"=\"*50 + \"\\n\"\n",
        "        for activity in self.activity_log[-10:]:  # Show last 10 activities\n",
        "            history_text += f\"üïê {activity['timestamp']}\\n\"\n",
        "            history_text += f\"üî∏ {activity['action']}\\n\"\n",
        "            if activity['details']:\n",
        "                history_text += f\"   {activity['details']}\\n\"\n",
        "            history_text += \"-\" * 30 + \"\\n\"\n",
        "\n",
        "        return history_text\n",
        "\n",
        "    def get_generation_history(self):\n",
        "        \"\"\"Get formatted generation history\"\"\"\n",
        "        if not self.generation_history:\n",
        "            return \"No generations yet\"\n",
        "\n",
        "        history_text = \"üé® Generation History:\\n\" + \"=\"*50 + \"\\n\"\n",
        "        for i, gen in enumerate(self.generation_history[-5:], 1):  # Show last 5 generations\n",
        "            history_text += f\"#{i} - {gen['timestamp']}\\n\"\n",
        "            history_text += f\"üìù Prompt: {gen['prompt'][:60]}...\\n\"\n",
        "            history_text += f\"üé≤ Seed: {gen['seed']} | ‚öôÔ∏è Steps: {gen['steps']}\\n\"\n",
        "            history_text += f\"üìê Size: {gen['size']} | ‚è±Ô∏è Time: {gen['time']}\\n\"\n",
        "            history_text += \"-\" * 40 + \"\\n\"\n",
        "\n",
        "        return history_text\n",
        "\n",
        "# Initialize generator\n",
        "generator = FastImageGenerator()\n",
        "\n",
        "# Enhanced CSS for better performance\n",
        "custom_css = \"\"\"\n",
        "body {\n",
        "    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
        "}\n",
        "\n",
        ".gradio-container {\n",
        "    background: transparent !important;\n",
        "}\n",
        "\n",
        ".gr-button {\n",
        "    border-radius: 20px !important;\n",
        "    font-weight: bold !important;\n",
        "    transition: all 0.2s ease !important;\n",
        "}\n",
        "\n",
        ".gr-button.primary {\n",
        "    background: linear-gradient(45deg, #4CAF50, #45a049) !important;\n",
        "    color: white !important;\n",
        "    border: none !important;\n",
        "}\n",
        "\n",
        ".gr-button.secondary {\n",
        "    background: linear-gradient(45deg, #2196F3, #1976D2) !important;\n",
        "    color: white !important;\n",
        "    border: none !important;\n",
        "}\n",
        "\n",
        ".generate-button {\n",
        "    background: linear-gradient(45deg, #FF5722, #F44336) !important;\n",
        "    color: white !important;\n",
        "    font-size: 18px !important;\n",
        "    padding: 15px 30px !important;\n",
        "    border-radius: 25px !important;\n",
        "    font-weight: bold !important;\n",
        "}\n",
        "\n",
        ".header {\n",
        "    text-align: center;\n",
        "    color: white;\n",
        "    padding: 20px;\n",
        "    background: rgba(255,255,255,0.1);\n",
        "    border-radius: 15px;\n",
        "    margin-bottom: 20px;\n",
        "    backdrop-filter: blur(10px);\n",
        "}\n",
        "\n",
        ".status-box {\n",
        "    background: rgba(255,255,255,0.1) !important;\n",
        "    color: white !important;\n",
        "    border-radius: 10px !important;\n",
        "    padding: 10px !important;\n",
        "    font-weight: bold !important;\n",
        "}\n",
        "\n",
        ".info-display {\n",
        "    background: rgba(255,255,255,0.1) !important;\n",
        "    color: white !important;\n",
        "    border-radius: 10px !important;\n",
        "    padding: 15px !important;\n",
        "    font-family: monospace !important;\n",
        "    font-size: 14px !important;\n",
        "    border-left: 4px solid #4CAF50 !important;\n",
        "}\n",
        "\n",
        ".history-display {\n",
        "    background: rgba(255,255,255,0.1) !important;\n",
        "    color: white !important;\n",
        "    border-radius: 10px !important;\n",
        "    padding: 15px !important;\n",
        "    font-family: monospace !important;\n",
        "    font-size: 12px !important;\n",
        "    border-left: 4px solid #2196F3 !important;\n",
        "    max-height: 300px !important;\n",
        "    overflow-y: auto !important;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Preset prompts for quick start\n",
        "preset_prompts = [\n",
        "    \"A majestic dragon soaring over mountains, fantasy digital art, highly detailed\",\n",
        "    \"Cyberpunk cityscape at night, neon lights, futuristic, 8k resolution\",\n",
        "    \"Beautiful cherry blossom tree in spring, soft lighting, photorealistic\",\n",
        "    \"Steampunk robot in Victorian setting, brass and copper, intricate details\",\n",
        "    \"Magical underwater city with glowing coral, fantasy art, vibrant colors\",\n",
        "    \"Space explorer on alien planet, sci-fi, dramatic lighting, cinematic\",\n",
        "    \"Ancient mystical forest with fairy lights, ethereal atmosphere, enchanting\",\n",
        "    \"Modern architectural marvel, glass and steel, sunset lighting, photography\"\n",
        "]\n",
        "\n",
        "def create_fast_interface():\n",
        "    \"\"\"Create optimized interface compatible with all Gradio versions\"\"\"\n",
        "\n",
        "    # Create interface using compatible syntax\n",
        "    with gr.Blocks(css=custom_css, title=\"Fast AI Image Generator\") as app:\n",
        "\n",
        "        # Header\n",
        "        gr.HTML(\"\"\"\n",
        "        <div class=\"header\">\n",
        "            <h1>üöÄ Fast AI Image Generator</h1>\n",
        "            <p>‚ö° Optimized for Speed | üé® Powered by Stable Diffusion</p>\n",
        "            <p>üåê <strong>Your public URL will appear in the console output below</strong></p>\n",
        "        </div>\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                # Model Section\n",
        "                gr.Markdown(\"### ü§ñ Model Control\")\n",
        "                load_model_btn = gr.Button(\"üöÄ Load Fast Model\", variant=\"primary\")\n",
        "                model_status = gr.Textbox(\n",
        "                    label=\"Model Status\",\n",
        "                    value=\"üî¥ Click 'Load Fast Model' to start\",\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "                # Quick Settings\n",
        "                gr.Markdown(\"### ‚öôÔ∏è Quick Settings\")\n",
        "                with gr.Row():\n",
        "                    width = gr.Slider(256, 1024, 512, step=64, label=\"Width\")\n",
        "                    height = gr.Slider(256, 1024, 512, step=64, label=\"Height\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    steps = gr.Slider(5, 25, 15, step=1, label=\"Steps (lower = faster)\")\n",
        "                    guidance = gr.Slider(3, 15, 7, step=0.5, label=\"Guidance Scale\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    seed = gr.Number(-1, label=\"Seed (-1 = random)\")\n",
        "                    batch_count = gr.Slider(1, 4, 1, step=1, label=\"Images Count\")\n",
        "\n",
        "                # Quick Actions\n",
        "                gr.Markdown(\"### üéØ Quick Actions\")\n",
        "                preset_dropdown = gr.Dropdown(\n",
        "                    choices=preset_prompts,\n",
        "                    label=\"Example Prompts\",\n",
        "                    value=preset_prompts[0]\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    use_preset_btn = gr.Button(\"Use Example\", variant=\"secondary\")\n",
        "                    random_btn = gr.Button(\"Random Prompt\", variant=\"secondary\")\n",
        "\n",
        "                # Activity History\n",
        "                gr.Markdown(\"### üìä My Activity\")\n",
        "                activity_btn = gr.Button(\"Show Activity\", variant=\"secondary\")\n",
        "                activity_display = gr.Textbox(\n",
        "                    label=\"Activity History\",\n",
        "                    lines=8,\n",
        "                    interactive=False,\n",
        "                    visible=False\n",
        "                )\n",
        "\n",
        "                # Generation History\n",
        "                history_btn = gr.Button(\"Show Generation History\", variant=\"secondary\")\n",
        "                history_display = gr.Textbox(\n",
        "                    label=\"Generation History\",\n",
        "                    lines=8,\n",
        "                    interactive=False,\n",
        "                    visible=False\n",
        "                )\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                # Prompts\n",
        "                gr.Markdown(\"### üí≠ Describe Your Image\")\n",
        "                prompt_input = gr.Textbox(\n",
        "                    label=\"Main Prompt\",\n",
        "                    placeholder=\"A beautiful landscape...\",\n",
        "                    lines=3,\n",
        "                    value=\"A beautiful sunset over mountains with a lake, digital art, highly detailed\"\n",
        "                )\n",
        "\n",
        "                negative_input = gr.Textbox(\n",
        "                    label=\"Negative Prompt (what to avoid)\",\n",
        "                    placeholder=\"blurry, low quality...\",\n",
        "                    lines=2,\n",
        "                    value=\"blurry, low quality, distorted, ugly, bad anatomy\"\n",
        "                )\n",
        "\n",
        "                # Generate Button\n",
        "                generate_btn = gr.Button(\"üé® Generate Images Fast!\", variant=\"primary\", elem_classes=\"generate-button\")\n",
        "\n",
        "                # Status Display\n",
        "                generation_info = gr.Textbox(\n",
        "                    label=\"Generation Status\",\n",
        "                    lines=8,\n",
        "                    interactive=False,\n",
        "                    value=\"üéØ Ready! Load model and click generate to start creating amazing images!\"\n",
        "                )\n",
        "\n",
        "        # Gallery\n",
        "        gr.Markdown(\"### üñºÔ∏è Generated Images\")\n",
        "        image_gallery = gr.Gallery(\n",
        "            label=\"Your Images\",\n",
        "            show_label=False,\n",
        "            elem_id=\"gallery\",\n",
        "            columns=2,\n",
        "            rows=2\n",
        "        )\n",
        "\n",
        "        # Event Handlers\n",
        "        load_model_btn.click(\n",
        "            generator.load_model,\n",
        "            outputs=[model_status]\n",
        "        )\n",
        "\n",
        "        generate_btn.click(\n",
        "            generator.generate_image,\n",
        "            inputs=[prompt_input, negative_input, width, height, steps, guidance, seed, batch_count],\n",
        "            outputs=[image_gallery, generation_info]\n",
        "        )\n",
        "\n",
        "        use_preset_btn.click(\n",
        "            lambda x: x,\n",
        "            inputs=[preset_dropdown],\n",
        "            outputs=[prompt_input]\n",
        "        )\n",
        "\n",
        "        random_btn.click(\n",
        "            generator.get_random_prompt,\n",
        "            outputs=[prompt_input]\n",
        "        )\n",
        "\n",
        "        activity_btn.click(\n",
        "            lambda: (generator.get_activity_history(), True),\n",
        "            outputs=[activity_display, activity_display]\n",
        "        )\n",
        "\n",
        "        history_btn.click(\n",
        "            lambda: (generator.get_generation_history(), True),\n",
        "            outputs=[history_display, history_display]\n",
        "        )\n",
        "\n",
        "    return app\n",
        "\n",
        "def launch_app():\n",
        "    \"\"\"Launch the application with URL display\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üöÄ LAUNCHING FAST AI IMAGE GENERATOR\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Create and launch\n",
        "    app = create_fast_interface()\n",
        "\n",
        "    print(\"\\nüì° Starting server with public URL...\")\n",
        "\n",
        "    # Launch with public sharing (Fixed: Removed invalid 'show_tips' argument)\n",
        "    app.launch(\n",
        "        share=True,\n",
        "        server_name=\"0.0.0.0\",\n",
        "        server_port=7864,\n",
        "        inbrowser=True,\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "# Run the application\n",
        "if __name__ == \"__main__\":\n",
        "    launch_app()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "id": "I259HAn9FxCO",
        "outputId": "98321b8f-dca1-4238-c4ed-21e16a1149e9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradio version: 5.31.0\n",
            "üöÄ Device: cuda\n",
            "üìä GPU: Tesla T4\n",
            "üíæ VRAM: 14.7 GB\n",
            "============================================================\n",
            "üöÄ LAUNCHING FAST AI IMAGE GENERATOR\n",
            "============================================================\n",
            "\n",
            "üì° Starting server with public URL...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://4af3f88a720b205bf6.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4af3f88a720b205bf6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}